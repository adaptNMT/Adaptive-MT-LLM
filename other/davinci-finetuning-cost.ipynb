{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMw7nNWT9QYDXi8t/sAEZ5h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ymoslem/Adaptive-MT-LLM/blob/main/other/davinci-finetuning-cost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cost estimation of fine-tuning Davinci GPT-3 model  \n",
        "\n",
        "See more [MT tutorials and notebooks](https://github.com/ymoslem). "
      ],
      "metadata": {
        "id": "aTAIh6bKudZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download sample files"
      ],
      "metadata": {
        "id": "kF6hEi3VDkAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download sample files\n",
        "\n",
        "# Source\n",
        "!wget -q --show-progress  https://raw.githubusercontent.com/ymoslem/Adaptive-MT-LLM/main/data/tico-19/tico-19-enes-dedup.en\n",
        "# Target\n",
        "!wget -q --show-progress https://raw.githubusercontent.com/ymoslem/Adaptive-MT-LLM/main/data/tico-19/tico-19-enes-dedup.es"
      ],
      "metadata": {
        "id": "g6GIHChqDma9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "iTSblKR9FH0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tiktoken installation"
      ],
      "metadata": {
        "id": "Fj0yOOknwEsK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX6a7GYOye8M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90734de8-4b9e-4ea0-e0d0-1953aadc449f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.7 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip3 install tiktoken -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 freeze | grep tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JU3xf-rsM44k",
        "outputId": "49956512-6453-48a9-e1a7-07320e0a5554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken==0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization functions"
      ],
      "metadata": {
        "id": "jYjgkPh5wQJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def encode(text):\n",
        "  enc = tiktoken.encoding_for_model(\"davinci\")\n",
        "  encoded = enc.encode(text)\n",
        "  # print(enc.n_vocab)\n",
        "  return encoded\n",
        "\n",
        "encode(\"This is a test!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-gf0YncykNs",
        "outputId": "f2a349dd-a9f1-4488-e876-c0d8ee333a4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1212, 318, 257, 1332, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch encoding function\n",
        "\n",
        "import functools\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def encode_batch(batch, num_threads=32):\n",
        "  encoder = functools.partial(encode)\n",
        "  with ThreadPoolExecutor(num_threads) as e:\n",
        "    return list(e.map(encoder, batch))"
      ],
      "metadata": {
        "id": "RDV8MqbQ7BZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test batch encoding\n",
        "batch = [\"Hello!\", \"this is a test.\", \"What about this somehow longer sentence?\"]\n",
        "encode_batch(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHAoe6oN74JP",
        "outputId": "bc1ec341-3dbc-4b3b-874e-a390ec3ea82f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[15496, 0],\n",
              " [5661, 318, 257, 1332, 13],\n",
              " [2061, 546, 428, 7599, 2392, 6827, 30]]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing the dataset and counting tokens\n",
        "\n",
        "When you prepare the training dataset, you might need to add more tokens for prompts, separators, etc. So, while you can have a rough estimation using a raw dataset, it is better to conduct this estimation after data preparation. See the official [fine-tuning guide](https://platform.openai.com/docs/guides/fine-tuning/advanced-usage)."
      ],
      "metadata": {
        "id": "UEg6DO8xwTYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count tokens in the source and target datasets\n",
        "\n",
        "# Change the source and target files\n",
        "source_file = \"tico-19-enes-dedup.en\"\n",
        "target_file = \"tico-19-enes-dedup.es\"\n",
        "\n",
        "# Tokenize the dataset and count tokens\n",
        "with open(source_file) as src, open(target_file) as tgt:\n",
        "  src_sents = src.readlines()\n",
        "  tgt_sents = tgt.readlines()\n",
        "\n",
        "  print(f\"Number of sentences: {len(src_sents)} + {len(tgt_sents)} = {len(src_sents) + len(tgt_sents)} \\n\")\n",
        "\n",
        "  src_sents_encoded = encode_batch(src_sents)\n",
        "  tgt_sents_encoded = encode_batch(tgt_sents)\n",
        "\n",
        "  out_src = []\n",
        "  for src_sent_encoded in src_sents_encoded:\n",
        "    out_src.extend(src_sent_encoded)\n",
        "  \n",
        "\n",
        "  out_tgt = []\n",
        "  for tgt_sent_encoded in tgt_sents_encoded:\n",
        "    out_tgt.extend(tgt_sent_encoded)\n",
        "  \n",
        "  print(f\"Source tokens: {len(out_src)}\")\n",
        "  print(f\"Target tokens: {len(out_tgt)}\")\n",
        "  print(f\"\\nTotal tokens {len(out_src) + len(out_tgt)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vk6Lt8PXFnuN",
        "outputId": "6560736b-444e-4a1f-959c-287035d6b6ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences: 3070 + 3070 = 6140 \n",
            "\n",
            "Source tokens: 96949\n",
            "Target tokens: 183786\n",
            "\n",
            "Total tokens 280735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cost estimation"
      ],
      "metadata": {
        "id": "yx6lFvD8wcR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimate cost of fine-tuning for 1 epoch\n",
        "\n",
        "tokens = len(out_src) + len(out_tgt)\n",
        "unit = 1000  # tokens\n",
        "unit_price = 0.03\n",
        "cost_per_epoch = round((tokens / unit) * unit_price, 2)\n",
        "\n",
        "print(f\"Cost estimation for 1 epoch: {cost_per_epoch} USD (excluding VAT)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsrRRlbHY3Nz",
        "outputId": "0932e3a5-2fc9-47af-c3ee-6ab91a296eed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost estimation for 1 epoch: 8.42 USD (excluding VAT)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimate cost of fine-tuning for a number of epochs\n",
        "\n",
        "# Change the number of training epochs\n",
        "n_epochs = 4\n",
        "\n",
        "total_cost = cost_per_epoch * n_epochs \n",
        "\n",
        "print(f\"Cost estimation for {n_epochs} epochs: {total_cost} USD (excluding VAT)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jpo4bxc2XOv",
        "outputId": "f3361669-5575-4a1f-c6eb-5b89346c4c35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost estimation for 4 epochs: 33.68 USD (excluding VAT)\n"
          ]
        }
      ]
    }
  ]
}